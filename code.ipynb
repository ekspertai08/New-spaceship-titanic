{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aaf80701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c5d8f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "852341d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = read_data('data/train.csv')\n",
    "df_test = read_data('data/test.csv')\n",
    "df_submision = read_data('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3720c9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collumn_split(df):\n",
    "\n",
    "    # Spliting 'PassengerId' collumn, creating new columns 'Group' and 'InGroup'\n",
    "    df['Group'] = df['PassengerId'].str.split('_').str[0].astype(int)\n",
    "    df['GroupSize'] = df.groupby('Group')['Group'].transform('size')\n",
    "    df['InGroup'] = df['GroupSize'] > 1\n",
    "\n",
    "    # Splitting 'Name' column, add new collumn 'WithFamily'\n",
    "    df['Name'] = df['Name'].fillna('unknown unknown')\n",
    "    df['Surname'] = df['Name'].str.split(' ').str[-1]\n",
    "    surname_counts = df['Surname'].value_counts()\n",
    "    if 'unknown' in surname_counts:\n",
    "        surname_counts = surname_counts.drop('unknown')\n",
    "    has_family_surname = df['Surname'].apply(lambda x: x != 'unknown' and surname_counts.get(x, 0) > 1)\n",
    "    df['WithFamily'] = (df['InGroup'] & has_family_surname)\n",
    "\n",
    "    # Spliting 'Cabin' collumn, creating new columns 'Deck', 'Num' and 'Side'\n",
    "    df['Cabin'] = df['Cabin'].fillna('unknown/unknown/unknown')\n",
    "    df['Deck'] = df['Cabin'].str.split('/').str[0]\n",
    "    df['CabinNumber'] = df['Cabin'].str.split('/').str[1]\n",
    "    df['Side'] = df['Cabin'].str.split('/').str[2]\n",
    "\n",
    "    # Add columns \"All_Expenses\" and 'HasExpenses'\n",
    "    df[['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']] = df[['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']].fillna(0)\n",
    "    df[\"All_Expenses\"] = df[['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']].sum(axis=1)\n",
    "    df['HasExpenses'] = df['All_Expenses'].apply(lambda x: 1 if x>0 else 0)\n",
    "\n",
    "    # Dropping splitted columns\n",
    "    df = df.drop(columns=['PassengerId', 'Cabin', 'Name'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "daac3020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unnecessary_columns(df):\n",
    "    df = df.drop(columns=['Group', 'GroupSize', 'InGroup', 'Surname', 'CabinNumber'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95093d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def converting_columns_to_correct_type(df):\n",
    "    \n",
    "    if target_column in df.columns.tolist():\n",
    "        df[target_column] = df[target_column].astype(bool)\n",
    "    for col in df.columns.tolist():\n",
    "        if col in categorical_columns:\n",
    "            df[col] = df[col].astype('object')\n",
    "        elif col in continuous_columns:\n",
    "            df[col] = df[col].astype('float32')\n",
    "        elif col in boolean_columns:\n",
    "            df[col] = df[col].astype(bool)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c106f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_missing_data_fill(df):\n",
    "    \n",
    "    for col in df.columns.tolist():\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            if col in categorical_columns:\n",
    "                df[col] = df[col].fillna(df[col].mode()[0])\n",
    "            elif col in continuous_columns:\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "            elif col in boolean_columns:\n",
    "                df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06eb4baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning_and_preparation(df):\n",
    "\n",
    "    df = collumn_split(df)\n",
    "    df = drop_unnecessary_columns(df)\n",
    "    df = simple_missing_data_fill(df)\n",
    "    df = converting_columns_to_correct_type(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a0d293b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(df):\n",
    "    \n",
    "    df = df.copy()\n",
    "    corr = df.select_dtypes(include=['number', 'bool']).corr()\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True, cbar_kws={\"shrink\": .8})\n",
    "    plt.title(\"Correlation Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38f12eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(df_train, df_test):\n",
    "    \n",
    "    df_train = pd.get_dummies(df_train, columns=categorical_columns, drop_first=True)\n",
    "    df_test = pd.get_dummies(df_test, columns=categorical_columns, drop_first=True)\n",
    "    df_test = df_test.reindex(columns=df_train.columns)\n",
    "    df_test = df_test.drop(columns=target_column)\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf831f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(df_train, df_test):\n",
    "    scaler = StandardScaler()\n",
    "    df_train[continuous_columns] = scaler.fit_transform(df_train[continuous_columns])\n",
    "    df_test[continuous_columns] = scaler.transform(df_test[continuous_columns])\n",
    "    \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8522648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_convert_for_tensorflow(df_train, df_test):\n",
    "    \n",
    "    df_train = df_train.astype('float32')\n",
    "    df_test = df_test.astype('float32')\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9e60c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_encoding_scaling_converting_for_tf(df_train, df_test):\n",
    "\n",
    "    df_train, df_test = encoding(df_train, df_test)\n",
    "    df_train, df_test = scaling(df_train, df_test)\n",
    "    df_train, df_test = last_convert_for_tensorflow(df_train, df_test)\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e95fe22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_low_correlation_features(df_train, df_test, threshold=0.1):\n",
    "\n",
    "    correlations = df_train.corr()[target_column].abs()\n",
    "\n",
    "    columns_to_keep = correlations[correlations >= threshold].index.tolist()\n",
    "    \n",
    "    # Make sure target column is in the columns to keep\n",
    "    if target_column not in columns_to_keep:\n",
    "        columns_to_keep.append(target_column)\n",
    "    \n",
    "    df_train = df_train[columns_to_keep]\n",
    "    columns_to_keep.remove(target_column)\n",
    "    df_test = df_test[columns_to_keep]\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5edf4214",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'Transported'\n",
    "categorical_columns = ['HomePlanet', 'Destination', 'Deck', 'Side']\n",
    "continuous_columns = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'All_Expenses', 'Age']\n",
    "boolean_columns = ['CryoSleep', 'VIP', 'WithFamily', 'HasExpenses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "088cb655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bj/0v2jl76905s95n66w11nry680000gn/T/ipykernel_75817/3261296494.py:10: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna(df[col].mode()[0])\n",
      "/var/folders/bj/0v2jl76905s95n66w11nry680000gn/T/ipykernel_75817/3261296494.py:10: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna(df[col].mode()[0])\n"
     ]
    }
   ],
   "source": [
    "df_train = data_cleaning_and_preparation(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae1263e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bj/0v2jl76905s95n66w11nry680000gn/T/ipykernel_75817/3261296494.py:10: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna(df[col].mode()[0])\n",
      "/var/folders/bj/0v2jl76905s95n66w11nry680000gn/T/ipykernel_75817/3261296494.py:10: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna(df[col].mode()[0])\n"
     ]
    }
   ],
   "source": [
    "df_test = data_cleaning_and_preparation(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0b4aa31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_encoding_scaling_converting_for_tf(df_train, df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "48a8a3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = drop_low_correlation_features(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0380a055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "def xgb_predictions(df_train, df_test):\n",
    "\n",
    "    X = df_train.copy()\n",
    "    y = X['Transported']\n",
    "    X = X.drop(columns='Transported')\n",
    "    X_train, X_val, y_train, y_val=train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300, 500],\n",
    "        'max_depth': [3, 4, 5, 6, 7],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'min_child_weight': [1, 3, 5, 7],\n",
    "        'gamma': [0, 0.1, 0.2, 0.5]\n",
    "    }\n",
    "\n",
    "    xgb1 = xgb.XGBClassifier(random_state=42)\n",
    "    random_search = RandomizedSearchCV(xgb1, param_distributions=param_grid, n_iter=20, scoring='roc_auc', cv=5, random_state=42, n_jobs=-1, verbose=1)\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Best parameters: {random_search.best_params_}\")\n",
    "    print(f\"Best ROC-AUC: {random_search.best_score_:.4f}\")\n",
    "\n",
    "    focused_param_grid = {\n",
    "        'n_estimators': [250, 300, 350],\n",
    "        'max_depth': [5, 6, 7],\n",
    "        'learning_rate': [0.05, 0.06, 0.07],\n",
    "        'subsample': [0.9, 1.0],\n",
    "        'colsample_bytree': [0.9, 1.0],\n",
    "        'min_child_weight': [1, 2, 3],\n",
    "        'gamma': [0.30, 0.35, 0.40]\n",
    "    }\n",
    "\n",
    "    xgb2 = xgb.XGBClassifier(random_state=42)\n",
    "    grid_search = GridSearchCV(xgb2, param_grid=focused_param_grid, scoring='roc_auc', cv=5, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best ROC-AUC: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "    best_model = random_search.best_estimator_\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    submision = best_model.predict(df_test)\n",
    "    df_submision['Transported'] = submision.astype(bool)\n",
    "    df_submision.to_csv('data/xgb_submisions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cab037f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers # type: ignore\n",
    "from tensorflow.keras.optimizers import Adam # type: ignore\n",
    "\n",
    "def simple_nn_predictions(df_train, df_test):\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "    X = df_train.copy()\n",
    "    y = X[target_column]\n",
    "    X = X.drop(columns=target_column)\n",
    "\n",
    "    def create_improved_model(optimizer=optimizer, activation='relu', neurons=64):\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.InputLayer(input_shape=(X.shape[1],)))\n",
    "        \n",
    "        model.add(layers.Dense(neurons))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Activation(activation))\n",
    "        model.add(layers.Dropout(0.3))\n",
    "        \n",
    "        model.add(layers.Dense(neurons // 2))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Activation(activation))\n",
    "        model.add(layers.Dropout(0.2))\n",
    "        \n",
    "        model.add(layers.Dense(neurons // 4))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Activation(activation))\n",
    "        \n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        model.compile(optimizer=optimizer, \n",
    "                    loss='binary_crossentropy', \n",
    "                    metrics=['accuracy', keras.metrics.AUC()])\n",
    "        return model\n",
    "\n",
    "    model = create_improved_model()\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, mode='min'),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=8, min_lr=1e-6),\n",
    "        keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')\n",
    "    ]\n",
    "\n",
    "    history = model.fit(X, y, epochs=150, batch_size=32, validation_split=0.20, verbose=1, callbacks=callbacks)\n",
    "\n",
    "    best_model = keras.models.load_model('best_model.h5')\n",
    "\n",
    "    submision = best_model.predict(df_test)\n",
    "    df_submision['Transported'] = (submision > 0.5).astype(bool)\n",
    "    df_submision.to_csv('data/nn_submisions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ac733f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers # type: ignore\n",
    "from tensorflow.keras.optimizers import Adam, AdamW # type: ignore\n",
    "import optuna\n",
    "\n",
    "def optuna_predictions(df_train, df_test):\n",
    "\n",
    "    X = df_train.copy()\n",
    "    y = X[target_column]\n",
    "    X = X.drop(columns=target_column)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    def create_model(optimizer='adam', activation='relu', neurons=64, layer_count=1, dropout=0.0, learning_rate=0.001, weight_decay=0.01):\n",
    "\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.InputLayer(shape=(X.shape[1],)))\n",
    "        for i in range(layer_count):\n",
    "            model.add(layers.Dense(neurons, activation=activation))\n",
    "            model.add(layers.Dropout(dropout))\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        if optimizer == 'adam':\n",
    "            optimizer = Adam(learning_rate=learning_rate)\n",
    "        elif optimizer == 'adamw':\n",
    "            optimizer = AdamW(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    # ADDED: Pyramidal model architecture\n",
    "    def create_pyramidal_model(optimizer='adam', neurons=16, dropout=0.2):\n",
    "\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.InputLayer(shape=(X.shape[1],)))\n",
    "        model.add(layers.Dense(neurons * 8))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Activation('relu'))\n",
    "        model.add(layers.Dropout(dropout))\n",
    "        model.add(layers.Dense(neurons * 4))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Activation('relu'))\n",
    "        model.add(layers.Dropout(dropout * 0.8))\n",
    "        model.add(layers.Dense(neurons * 2))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Activation('relu'))\n",
    "        model.add(layers.Dropout(dropout * 0.6))\n",
    "        model.add(layers.Dense(neurons))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Activation('relu'))\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        if optimizer == 'adam':\n",
    "            optimizer = Adam(learning_rate=0.001)\n",
    "        elif optimizer == 'adamw':\n",
    "            optimizer = AdamW(learning_rate=0.001, weight_decay=0.01)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    # ADDED: Residual model architecture\n",
    "    def create_residual_model(optimizer='adam', neurons=16, layer_count=4, dropout=0.2):\n",
    "\n",
    "        inputs = layers.Input(shape=(X.shape[1],))\n",
    "        x = layers.LayerNormalization()(inputs)\n",
    "        for i in range(layer_count):\n",
    "            residual = x if i > 0 else None\n",
    "            x = layers.Dense(neurons)(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            x = layers.Activation('relu')(x)\n",
    "            x = layers.Dropout(dropout)(x)\n",
    "            if residual is not None and x.shape[-1] == residual.shape[-1]:\n",
    "                x = layers.Add()([x, residual])\n",
    "        outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        if optimizer == 'adam':\n",
    "            optimizer = Adam(learning_rate=0.001)\n",
    "        elif optimizer == 'adamw':\n",
    "            optimizer = AdamW(learning_rate=0.001, weight_decay=0.01)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    # ADDED: Ensemble model architecture\n",
    "    def create_ensemble_model(optimizer='adam', neurons=16, layer_count=4, dropout=0.2):\n",
    "\n",
    "        inputs = layers.Input(shape=(X.shape[1],))\n",
    "        x1 = layers.Dense(neurons * 4, activation='relu')(inputs)\n",
    "        x1 = layers.BatchNormalization()(x1)\n",
    "        x1 = layers.Dropout(dropout)(x1)\n",
    "        x1 = layers.Dense(neurons * 2, activation='relu')(x1)\n",
    "        x2 = inputs\n",
    "        for i in range(layer_count):\n",
    "            x2 = layers.Dense(neurons, activation='relu')(x2)\n",
    "            x2 = layers.BatchNormalization()(x2)\n",
    "            x2 = layers.Dropout(dropout)(x2)\n",
    "        merged = layers.Concatenate()([x1, x2])\n",
    "        merged = layers.Dense(neurons // 2, activation='relu')(merged)\n",
    "        merged = layers.BatchNormalization()(merged)\n",
    "        outputs = layers.Dense(1, activation='sigmoid')(merged)\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        if optimizer == 'adam':\n",
    "            optimizer = Adam(learning_rate=0.001)\n",
    "        elif optimizer == 'adamw':\n",
    "            optimizer = AdamW(learning_rate=0.001, weight_decay=0.01)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, mode='min'),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=8, min_lr=1e-6),\n",
    "    ]\n",
    "    \n",
    "    def objective(trial):\n",
    "\n",
    "        # ADDED: Model type selection\n",
    "        model_type = trial.suggest_categorical('model_type', ['standard', 'pyramidal'])\n",
    "        optimizer = trial.suggest_categorical('optimizer', ['adamw'])\n",
    "        neurons = trial.suggest_int('neurons', 48, 80, step=8)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [40, 48, 56])\n",
    "        layer_count = trial.suggest_int('layer_count', 3, 5, step=1)\n",
    "        dropout = trial.suggest_float(\"dropout\", 0.1, 0.2, step=0.025)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 0.005, 0.015, log=True)\n",
    "        weight_decay = trial.suggest_float(\"weight_decay\", 0.001, 0.05, log=True)\n",
    "        \n",
    "        # ADDED: Model selection based on model_type\n",
    "        if model_type == 'standard':\n",
    "            model = create_model(optimizer=optimizer, neurons=neurons, layer_count=layer_count, dropout=dropout)\n",
    "        elif model_type == 'pyramidal':\n",
    "            model = create_pyramidal_model(optimizer=optimizer, neurons=neurons, dropout=dropout)\n",
    "        elif model_type == 'residual':\n",
    "            model = create_residual_model(optimizer=optimizer, neurons=neurons, layer_count=layer_count, dropout=dropout)\n",
    "        else:  # ensemble\n",
    "            model = create_ensemble_model(optimizer=optimizer, neurons=neurons, layer_count=layer_count, dropout=dropout)\n",
    "        \n",
    "        if optimizer == 'adamw':\n",
    "            model.optimizer.learning_rate = learning_rate\n",
    "            model.optimizer.weight_decay = weight_decay\n",
    "\n",
    "        history = model.fit(X_train, y_train, epochs=100, batch_size=batch_size, validation_split=0.20, verbose=1, callbacks=callbacks)\n",
    "        loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "        return accuracy\n",
    "    \n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=20, n_jobs=1)\n",
    "    print(\"Best hyperparameters: \", study.best_params)\n",
    "    print(\"Best trial: \", study.best_trial)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    model_type = best_params['model_type']\n",
    "    optimizer = best_params['optimizer']\n",
    "    neurons = best_params['neurons']\n",
    "    batch_size = best_params['batch_size']\n",
    "    layer_count = best_params['layer_count']\n",
    "    dropout = best_params['dropout']\n",
    "    learning_rate = best_params['learning_rate']\n",
    "\n",
    "    if model_type == 'standard':\n",
    "        best_model = create_model(optimizer=optimizer, neurons=neurons, layer_count=layer_count, dropout=dropout)\n",
    "    elif model_type == 'pyramidal':\n",
    "        best_model = create_pyramidal_model(optimizer=optimizer, neurons=neurons, dropout=dropout)\n",
    "    elif model_type == 'residual':\n",
    "        best_model = create_residual_model(optimizer=optimizer, neurons=neurons, layer_count=layer_count, dropout=dropout)\n",
    "    else:  # ensemble\n",
    "        best_model = create_ensemble_model(optimizer=optimizer, neurons=neurons, layer_count=layer_count, dropout=dropout)\n",
    "\n",
    "    best_model.fit(X, y, epochs=100, batch_size=batch_size, validation_split=0.20, verbose=1, callbacks=callbacks)\n",
    "\n",
    "    submision = best_model.predict(df_test)\n",
    "\n",
    "    df_submision['Transported'] = (submision > 0.5).astype(bool)\n",
    "    df_submision.to_csv('data/optuna_submisions.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f51dd95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(history.history['loss'], label='Training Loss')\n",
    "    ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax1.set_title('Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    ax2.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax2.set_title('Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c47472e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_predictions(df_train, df_test)\n",
    "# simple_nn_predictions(df_train, df_test)\n",
    "# optuna_predictions(df_train, df_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
